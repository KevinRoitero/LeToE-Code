{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collections = ['TREC3', 'TREC5', 'TREC6', 'TREC7', 'TREC8', 'TREC2001', 'R04', 'TB06', 'TB06M', 'WEB14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Depth to which we want the runs to be cut\n",
    "'''\n",
    "pool_D = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create \"annotated\" runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Support methods and variables used in building the runs and computing WUC\n",
    "'''\n",
    "global weight\n",
    "weight = np.array([])\n",
    "\n",
    "# Custom function to add to a dictionary\n",
    "def myadd(d, key, system, position):\n",
    "    if key in d.keys():\n",
    "        d[key][str(system)] = position\n",
    "    else:\n",
    "        d[key] = {system: position}\n",
    "\n",
    "# Compute weights of reference documents according to WUC formula\n",
    "def get_weight(j):\n",
    "    if j in weight:\n",
    "        return weight.get(j)\n",
    "    else:\n",
    "        for m in range(1, 1001):\n",
    "            for k in range(1, 5):\n",
    "                if (m * 5) == j:\n",
    "                    return round(zeta_function(200) - zeta_function(m - 1), 2)\n",
    "                if ((5 * m) - k) == j:\n",
    "                    return round(get_weight(5 * m) - (1 / m) + (5 / j), 2)\n",
    "\n",
    "# Compute Zeta function (used in computing the weights)\n",
    "def zeta_function(m):\n",
    "    res = 1\n",
    "    if m == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in range(1, m + 1):\n",
    "            res = res + (1 / i)\n",
    "    return res\n",
    "\n",
    "# Set up a dictionary that, for each position, returns the corresponding weight\n",
    "for i in range(1, 1001):\n",
    "    weight = np.append(weight, get_weight(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Method that reads the runs, build dictionaries used for WUC implementations and saves those dictionaries in .pickles\n",
    "files\n",
    "'''\n",
    "def get_runs(systems, topics, RUN_PATH, pool_depth):\n",
    "    # Load the runs' filepatsh in a list\n",
    "    files = [f for f in listdir(RUN_PATH) if (isfile(join(RUN_PATH, f))) & (f != \".DS_Store\")]\n",
    "    \n",
    "    df_run = pd.DataFrame(columns=['topic', 'system', 'rank_list'])\n",
    "    df_doc_sys = pd.DataFrame(columns=['topic', 'doc_system_dict'])\n",
    "    df_doc_score = pd.DataFrame(columns=['topic', 'doc_system_dict'])\n",
    "    \n",
    "    # Read the runs\n",
    "    for i_file, file in enumerate(files):\n",
    "        system = file.replace(\"input.\",\"\").replace(\".gz\",\"\")\n",
    "        if system in systems:\n",
    "            print(\"system {}/{}, \".format(i_file + 1, len(files)), end='')\n",
    "            file_content = pd.read_csv(join(RUN_PATH, file), compression='gzip', header=None, sep='\\t')\n",
    "            file_content.columns  = ['topic', 'zero', 'doc', 'rank', 'rel_value', 'system']\n",
    "            file_content.drop('zero', 1, inplace=True)\n",
    "            file_content['topic'] = file_content['topic'].astype(str)\n",
    "            file_content['system'] = file_content['system'].astype(str)\n",
    "            file_content.set_index('topic', inplace=True)\n",
    "            \n",
    "            # For each topic, build the dictionary containing the set of documents retrieved by a system and the\n",
    "            # position where they have been retrieved and the dictionary containing the set of documents retrieved\n",
    "            # by a system and their normalized relevance score\n",
    "            for i, topic in enumerate(topics):\n",
    "                if i_file == 0:\n",
    "                    dicti = {}\n",
    "                    dictib = {}\n",
    "                else:\n",
    "                    dicti = df_doc_sys[df_doc_sys['topic'] == str(topic)]['doc_system_dict'].values[0]\n",
    "                    dictib = df_doc_score[df_doc_score['topic'] == str(topic)]['doc_system_dict'].values[0]\n",
    "                file_content_top = file_content.loc[str(topic)]\n",
    "                file_content_top = file_content_top.head(pool_depth)\n",
    "                try:\n",
    "                    file_content_top.reset_index(inplace=True)\n",
    "                except:\n",
    "                    file_content_top = file_content_top.to_frame().T\n",
    "                file_content_top.set_index('doc', inplace=True)\n",
    "                df_run.loc[len(df_run)] = [topic, system, file_content_top.index.values]\n",
    "                rel_value_mean = file_content_top['rel_value'].mean()\n",
    "                rel_value_std = file_content_top['rel_value'].std()\n",
    "                if rel_value_std == 0 or math.isnan(rel_value_std):\n",
    "                    rel_value_std = 1\n",
    "                if math.isnan(rel_value_mean):\n",
    "                    rel_value_mean = 0\n",
    "                    \n",
    "                for posizione, doc in enumerate(file_content_top.index.values):\n",
    "                    # WUC V4 uses a \"normalized score\", we interpret this as the normalized relevance score of the\n",
    "                    # documents retrieved from a system (this value can be found in the runs and varies from system to\n",
    "                    # system)\n",
    "                    rel_value = file_content_top.loc[doc]['rel_value']\n",
    "                    rel_value = round(((rel_value - rel_value_mean) / rel_value_std), 3)\n",
    "                    if math.isnan(rel_value):\n",
    "                        rel_value = 0\n",
    "                    myadd(dicti, doc, str(system), int(posizione + 1))\n",
    "                    myadd(dictib, doc, str(system), rel_value)\n",
    "                \n",
    "                df_doc_sys.loc[len(df_doc_sys)] = [topic, dicti]\n",
    "                df_doc_score.loc[len(df_doc_score)] = [topic, dictib]\n",
    "    \n",
    "    # Save the dictionaries in .pickles files      \n",
    "    df_run.set_index(['topic', 'system'], inplace=True)\n",
    "    df_doc_sys = df_doc_sys.groupby('topic').last()\n",
    "    df_run.to_pickle('../../pickles/pickles_results/{}_WUC_df_run.pickle'.format(COLLECTION))\n",
    "    df_doc_sys.to_pickle('../../pickles/pickles_results/{}_WUC_df_doc_sys.pickle'.format(COLLECTION))\n",
    "    df_doc_score.to_pickle('../../pickles/pickles_results/{}_WUC_df_doc_score.pickle'.format(COLLECTION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create the dictionaries and save them\n",
    "'''\n",
    "for COLLECTION in COLLECTIONS:\n",
    "    print('Working on {}'.format(COLLECTION))\n",
    "    \n",
    "    # Real table and run path\n",
    "    CSV_TABLE = '../../src/Tables/{}.csv'.format(COLLECTION)\n",
    "    RUN_PATH = '../run/{}-input/'.format(COLLECTION)\n",
    "    \n",
    "    # Read real table\n",
    "    real_table = pd.read_csv(CSV_TABLE, sep=',', header=0, index_col=0)\n",
    "    systems = real_table.index.values\n",
    "    topics = real_table.columns.values\n",
    "    \n",
    "    get_runs(systems, topics, RUN_PATH, pool_D)\n",
    "    \n",
    "    print('done for {}'.format(COLLECTION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute WUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Compute WUC for each collection\n",
    "'''\n",
    "for COLLECTION in COLLECTIONS:\n",
    "    print('Working on {}'.format(COLLECTION))\n",
    "    \n",
    "    # Real table path\n",
    "    CSV_TABLE = '../../src/Tables/{}.csv'.format(COLLECTION)\n",
    "    \n",
    "    # Read real table and get the topics' numbers\n",
    "    real_table = pd.read_csv(CSV_TABLE, sep=',', header=0, index_col=0)\n",
    "    topics = real_table.columns.values\n",
    "    \n",
    "    # Read the previously computed dictionaries\n",
    "    df_run = pd.read_pickle('../../pickles/pickles_results/{}_WUC_df_run.pickle'.format(COLLECTION))\n",
    "    df_doc_sys = pd.read_pickle('../../pickles/pickles_results/{}_WUC_df_doc_sys.pickle'.format(COLLECTION))\n",
    "    df_doc_score = pd.read_pickle('../../pickles/pickles_results/{}_WUC_df_doc_score.pickle'.format(COLLECTION))\n",
    "    df_doc_score.set_index('topic', inplace=True)\n",
    "    \n",
    "    df_total = pd.DataFrame(columns=['topic', 'system', 'BASIC', 'V1', 'V2', 'V3', 'V4'])\n",
    "    \n",
    "    # For each topic, compute the five WUC variants for each system\n",
    "    for i_topic, topic in enumerate(topics):\n",
    "        df_run_top = df_run.loc[topic]\n",
    "        dict_doc_sys_top = df_doc_sys.loc[topic].values[0]\n",
    "        dict_doc_score_top = df_doc_score.loc[topic].values[0][0]\n",
    "        \n",
    "        for system, ranked_list in df_run_top.iterrows():\n",
    "            ranked_list = ranked_list[0]\n",
    "            \n",
    "            # Keep only the set of documents retrieved by every system except the one considered\n",
    "            dict_doc_sys_top_count = dict_doc_sys_top.copy()\n",
    "            dict_doc_score_top_count = dict_doc_score_top.copy()\n",
    "            unwanted = set(dict_doc_sys_top_count.keys()) - set(ranked_list)\n",
    "            for unwanted_key in unwanted: del dict_doc_sys_top_count[unwanted_key]\n",
    "            for unwanted_key in unwanted: del dict_doc_score_top_count[unwanted_key]\n",
    "            \n",
    "            sum_basic = 0\n",
    "            sum_V1 = 0\n",
    "            sum_V2 = 0\n",
    "            sum_V3 = 0\n",
    "            sum_V4 = 0\n",
    "            \n",
    "            # Compute WUC values for each retrieved document\n",
    "            for i_doc, dict_syst_rank in enumerate(dict_doc_sys_top_count.values()):\n",
    "                # WUC Basic counts how many systems retrieve a specific document\n",
    "                ref_count = len(dict_syst_rank) - 1\n",
    "                sum_basic += ref_count\n",
    "                \n",
    "                # WUC V2 adds a weight to the number of systems that retrieve a specific document\n",
    "                sum_V2 += (ref_count * weight[i_doc])\n",
    "\n",
    "                for sys_name, rank_of_doc_retr in dict_syst_rank.items():\n",
    "                    if sys_name != system:\n",
    "                        # WUC V1 weights the rank at which a document is retrieved\n",
    "                        sum_V1 += (1501 - rank_of_doc_retr)\n",
    "                        # WUC V3 weights both the rank at which a document is retrieved and the rank at which\n",
    "                        # the document is found in the specific system's set of retrieved documents\n",
    "                        sum_V3 += (1501 - rank_of_doc_retr) * weight[i_doc]\n",
    "                                   \n",
    "            for i_doc, dict_syst_score in enumerate(dict_doc_score_top_count.values()):\n",
    "                for sys_name, score_of_doc_retr in dict_syst_score.items():\n",
    "                    if sys_name != system:\n",
    "                        # WUC V3 weights the normalized score of a document according to its rank in the specific\n",
    "                        # system's set of retrieved documents\n",
    "                        sum_V4 += score_of_doc_retr * weight[i_doc]\n",
    "\n",
    "            df_total.loc[len(df_total)] = [topic, system, sum_basic, sum_V1, sum_V2, sum_V3, sum_V4]\n",
    "    \n",
    "    # Split the dataframe containing all methods into single tables, one for each WUC version\n",
    "    basic = df_total.pivot('system', 'topic', 'BASIC')\n",
    "    v1 = df_total.pivot('system', 'topic', 'V1')\n",
    "    v2 = df_total.pivot('system', 'topic', 'V2')\n",
    "    v3 = df_total.pivot('system', 'topic', 'V3')\n",
    "    v4 = df_total.pivot('system', 'topic', 'V4')\n",
    "\n",
    "    # Save WUC tables to csv\n",
    "    basic.to_csv('../../pickles/WUC_1000_bis/Table/{}_BASIC.csv'.format(COLLECTION), index=True, header=True)\n",
    "    v1.to_csv('../../pickles/WUC_1000_bis/Table/{}_V1.csv'.format(COLLECTION),index=True, header=True)\n",
    "    v2.to_csv('../../pickles/WUC_1000_bis/Table/{}_V2.csv'.format(COLLECTION),index=True, header=True)\n",
    "    v3.to_csv('../../pickles/WUC_1000_bis/Table/{}_V3.csv'.format(COLLECTION),index=True, header=True)\n",
    "    v4.to_csv('../../pickles/WUC_1000_bis/Table/{}_V4.csv'.format(COLLECTION),index=True, header=True)\n",
    "    \n",
    "    print('\\ndone for {}.'.format(COLLECTION))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
